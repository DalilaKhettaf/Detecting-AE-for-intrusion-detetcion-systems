{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Test"
      ],
      "metadata": {
        "id": "VpgYS-kG0WX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://drive.google.com/uc?id=1Cw2E09wgzQdFxPph8KrSB8WCZn71u_XC\" height = \"500\" >\n"
      ],
      "metadata": {
        "id": "G131KDQ3Ibd_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVhOE3vt0RdZ"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import numpy as np\n",
        "from sys import stdout\n",
        "from sklearn.metrics import pairwise_kernels\n",
        "import tensorflow as tf\n",
        "\n",
        "#\n",
        "#taken from https://github.com/emanuele/kernel_two_sample_test\n",
        "#\n",
        "\n",
        "def MMD2u(K, m, n):\n",
        "    \"\"\"The MMD^2_u unbiased statistic.\n",
        "    \"\"\"\n",
        "    Kx = K[:m, :m]\n",
        "    Ky = K[m:, m:]\n",
        "    Kxy = K[:m, m:]\n",
        "    return 1.0 / (m * (m - 1.0)) * (Kx.sum() - Kx.diagonal().sum()) + \\\n",
        "        1.0 / (n * (n - 1.0)) * (Ky.sum() - Ky.diagonal().sum()) - \\\n",
        "        2.0 / (m * n) * Kxy.sum()\n",
        "\n",
        "\n",
        "def compute_null_distribution(K, m, n, iterations=10000, verbose=False,\n",
        "                              random_state=None, marker_interval=1000):\n",
        "    \"\"\"Compute the bootstrap null-distribution of MMD2u.\n",
        "    \"\"\"\n",
        "    if type(random_state) == type(np.random.RandomState()):\n",
        "        rng = random_state\n",
        "    else:\n",
        "        rng = np.random.RandomState(random_state)\n",
        "\n",
        "    mmd2u_null = np.zeros(iterations)\n",
        "    for i in range(iterations):\n",
        "        if verbose and (i % marker_interval) == 0:\n",
        "            print(i),\n",
        "            stdout.flush()\n",
        "        idx = rng.permutation(m+n)\n",
        "        K_i = K[idx, idx[:, None]]\n",
        "        mmd2u_null[i] = MMD2u(K_i, m, n)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\")\n",
        "\n",
        "    return mmd2u_null\n",
        "\n",
        "\n",
        "def compute_null_distribution_given_permutations(K, m, n, permutation,\n",
        "                                                 iterations=None):\n",
        "    \"\"\"Compute the bootstrap null-distribution of MMD2u given\n",
        "    predefined permutations.\n",
        "    Note:: verbosity is removed to improve speed.\n",
        "    \"\"\"\n",
        "    if iterations is None:\n",
        "        iterations = len(permutation)\n",
        "\n",
        "    mmd2u_null = np.zeros(iterations)\n",
        "    for i in range(iterations):\n",
        "        idx = permutation[i]\n",
        "        K_i = K[idx, idx[:, None]]\n",
        "        mmd2u_null[i] = MMD2u(K_i, m, n)\n",
        "\n",
        "    return mmd2u_null\n",
        "\n",
        "def makeScaleMatrix(num_gen, num_orig):\n",
        "        # first 'N' entries have '1/N', next 'M' entries have '-1/M'\n",
        "        s1 =  tf.constant(1.0 / num_gen, shape = [num_gen, 1])\n",
        "        s2 = -tf.constant(1.0 / num_orig, shape = [num_orig, 1])\n",
        "\n",
        "        return tf.concat(0, [s1, s2])\n",
        "\n",
        "def MMD(x1, x2, sess,sigma = [80]):\n",
        "        x1 = tf.cast(x1, tf.float32)\n",
        "        x2 = tf.cast(x2, tf.float32)\n",
        "        # concatenation of the two datas\n",
        "        # first 'N' rows are the generated ones, next 'M' are from the data\n",
        "        X = tf.concat(0, [x1, x2])\n",
        "        # dot product between all combinations of rows in 'X'\n",
        "        XX = tf.matmul(X, tf.transpose(X))\n",
        "\n",
        "        # dot product of rows with themselves\n",
        "        X2 = tf.reduce_sum(X * X, 1, keep_dims = True)\n",
        "\n",
        "        # exponent entries of the RBF kernel (without the sigma) for each\n",
        "        # combination of the rows in 'X'\n",
        "        # -0.5 * (x^Tx - 2*x^Ty + y^Ty)\n",
        "        exponent = XX - 0.5 * X2 - 0.5 * tf.transpose(X2)\n",
        "\n",
        "        # scaling constants for each of the rows in 'X'\n",
        "        s = makeScaleMatrix(np.shape(sess.run(x1))[0], np.shape(sess.run(x2))[0])\n",
        "        # scaling factors of each of the kernel values, corresponding to the\n",
        "        # exponent values\n",
        "        S = tf.matmul(s, tf.transpose(s))\n",
        "        S = tf.cast(S, tf.float32)\n",
        "        loss = 0\n",
        "        # for each bandwidth parameter, compute the MMD value and add them all\n",
        "        for i in range(len(sigma)):\n",
        "            # kernel values for each combination of the rows in 'X' \n",
        "            kernel_val = tf.exp(1.0 / sigma[i] * exponent)\n",
        "            kernel_val = tf.cast(kernel_val, tf.float32)\n",
        "            kernel_val = S * kernel_val\n",
        "        return sess.run(kernel_val)\n",
        "\n",
        "def kernel_two_sample_test(X, Y, kernel_function='rbf', iterations=10000,\n",
        "                           verbose=False, random_state=None, myKernel=False, sess=None, **kwargs):\n",
        "    \"\"\"Compute MMD^2_u, its null distribution and the p-value of the\n",
        "    kernel two-sample test.\n",
        "    Note that extra parameters captured by **kwargs will be passed to\n",
        "    pairwise_kernels() as kernel parameters. E.g. if\n",
        "    kernel_two_sample_test(..., kernel_function='rbf', gamma=0.1),\n",
        "    then this will result in getting the kernel through\n",
        "    kernel_function(metric='rbf', gamma=0.1).\n",
        "    \"\"\"\n",
        "    m = len(X)\n",
        "    n = len(Y)\n",
        "    XY = np.vstack([X, Y])\n",
        "    if myKernel:\n",
        "       K = MMD(X,Y,sess) \n",
        "    else:\n",
        "       K = pairwise_kernels(XY, metric=kernel_function, **kwargs)\n",
        "    mmd2u = MMD2u(K, m, n)\n",
        "    if verbose:\n",
        "        print(\"MMD^2_u = %s\" % mmd2u)\n",
        "        print(\"Computing the null distribution.\")\n",
        "\n",
        "    mmd2u_null = compute_null_distribution(K, m, n, iterations,\n",
        "                                           verbose=verbose,\n",
        "                                           random_state=random_state)\n",
        "    p_value = max(1.0/iterations, (mmd2u_null > mmd2u).sum() /\n",
        "                  float(iterations))\n",
        "    if verbose:\n",
        "        print(\"p-value ~= %s \\t (resolution : %s)\" % (p_value, 1.0/iterations))\n",
        "    return mmd2u, mmd2u_null, p_value\n",
        "'''\n",
        "given one hot or labels, split dataset given the classes\n",
        "@param x: data\n",
        "@param y:labels\n",
        "'''\n",
        "def get_classes(x,y):\n",
        "   classes = []\n",
        "   #deal with one hot\n",
        "   if len(np.shape(y))>1:\n",
        "      num_classes = np.shape(y)[1]\n",
        "      indexes = np.zeros((num_classes))\n",
        "      for i in range(num_classes):\n",
        "         size = np.hstack([int(np.sum(y[:,i])),np.shape(x)[1:]])\n",
        "         classes.append(np.zeros((size)))\n",
        "      for i in range(np.shape(y)[0]):\n",
        "         curclass = np.argmax(y[i,:])  \n",
        "         classes[curclass][indexes[curclass],:]=x[i,:]\n",
        "         indexes[curclass] = indexes[curclass] +1\n",
        "   #deal with other\n",
        "   else: \n",
        "      num_classes = np.int_(np.max(y))+1\n",
        "      print (num_classes)\n",
        "      indexes = np.zeros((num_classes))\n",
        "      for i in range(num_classes):\n",
        "         size = np.hstack([int(np.sum(y==i)),np.shape(x)[1:]])\n",
        "         classes.append(np.zeros((size)))\n",
        "         #print(size)\n",
        "      for i in range(np.shape(y)[0]): \n",
        "         curclass = np.int_(y[i])  \n",
        "         classes[curclass][int(indexes[curclass])]=x[i,:]\n",
        "         indexes[curclass] = indexes[curclass] +1\n",
        "   return classes  \n",
        "'''\n",
        "sample randomly from given data\n",
        "@param x1: 1. data\n",
        "@param y1: 1. labels\n",
        "@param x2: 2. data\n",
        "@param y2: 2. labels\n",
        "@param size: size of resulting samples \n",
        "'''\n",
        "def getSamples(x1,y1,x2,y2,size=200):\n",
        "       indices1 = np.arange(np.shape(x1)[0])\n",
        "       np.random.shuffle(indices1)\n",
        "       indices2 = np.arange(np.shape(x2)[0])\n",
        "       np.random.shuffle(indices2)\n",
        "       return x1[indices1[0:size]],y1[indices1[0:size]],x2[indices2[0:size]],y2[indices2[0:size]]\n",
        "'''\n",
        "given several samples, compose a mixed sample for given ratio\n",
        "@param x1: first data\n",
        "@param x2:second data\n",
        "@param size1: number of datapoints from first data\n",
        "@param size2: number of datapoints from second sample\n",
        "@param ratio: ratio that first data has towards second data\n",
        "'''\n",
        "# Ratio of x2\n",
        "def samplestats(x1,x2,size1,size2,ratio):\n",
        "    if ratio==1.0:\n",
        "       indices1 = np.arange(np.shape(x1)[0])\n",
        "       np.random.shuffle(indices1)\n",
        "       indices2 = np.arange(np.shape(x2)[0])\n",
        "       np.random.shuffle(indices2)\n",
        "       return x2[indices2[0:size2]]\n",
        "    elif ratio>0.0 and ratio <1.0:\n",
        "       indices1 = np.arange(np.shape(x1)[0])\n",
        "       np.random.shuffle(indices1)\n",
        "       indices2 = np.arange(np.shape(x2)[0])\n",
        "       np.random.shuffle(indices2)\n",
        "       t = list(np.shape(x2))\n",
        "              # 0.3. 30.            70\n",
        "       t[0] = int(size2*ratio)+int(size1*(1.0-ratio))\n",
        "       t = tuple(t)\n",
        "       xt2 = np.zeros((t))\n",
        "       xt2[0:int(size2*ratio)] = x2[indices2[0:int(size2*ratio)]]\n",
        "       xt2[int(size2*ratio):] = x1[indices1[0:int(size1*(1.0-ratio))]]\n",
        "       indices1 = np.arange(np.shape(x1)[0])\n",
        "       np.random.shuffle(indices1)     \n",
        "       return xt2\n",
        "    else:\n",
        "       print('no value between 0 and 1.0 given!')\n",
        "'''\n",
        "sample from data randomly and observe p-value\n",
        "@param x1: first sample\n",
        "@param x2: second sample\n",
        "@param size1: size of sample for first data for test\n",
        "@param size2: size of second sample for second data for test\n",
        "@param ratio: for blending experiments only, define ratio of blending\n",
        "@param runs: times the test is run for newly sampled samples given setting\n",
        "'''\n",
        "def compute_stats(x1,x2,size1, size2,ratio, runs):\n",
        "    ret = np.zeros((runs))\n",
        "    for i in range(runs):\n",
        "       #print(np.shape(x1))\n",
        "       #print(np.shape(x2))\n",
        "       #xt1,xt2 = samplestats(x1,x2,size1,size2,ratio)\n",
        "       _, _, p_value = kernel_two_sample_test(x1,x2)\n",
        "       ret[i] = p_value\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outlier Class"
      ],
      "metadata": {
        "id": "7HFKGr8q1NC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concept\n",
        "<img src = \"https://drive.google.com/uc?id=1xg9jBacagaN8DpYjt0z4M-7vFGkVLyXS\" height = \"400\">\n",
        "<br>\n",
        "<br>\n",
        "<h2>Données</h2>\n",
        "<img src = \"https://drive.google.com/uc?id=13w1ipz9MLWy3C1k2Xwrf7tza5CnF_Fl4\" width = \"1000\">\n"
      ],
      "metadata": {
        "id": "-aJPO7DzBqdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path_df = '/content/drive/My Drive/****/Adversarial samples/Crafted train/train_DeepFool.npy'\n",
        "path_fgsm = '/content/drive/My Drive/****/Adversarial samples/Crafted train/train_FGSM0.0001.npy'\n",
        "path_jsma = '/content/drive/My Drive/****/Adversarial samples/Crafted train/train_JSMA.npy'\n",
        "path_cw = '/content/drive/My Drive/****/Adversarial samples/Crafted train/train_carliniL2.npy'\n",
        "path_pgd = '/content/drive/My Drive/****/Adversarial samples/Crafted train/train_pgd.npy'\n",
        "\n",
        "df = np.load(path_df)\n",
        "fgsm = np.load(path_fgsm)\n",
        "jsma = np.load(path_jsma)\n",
        "cw = np.load(path_cw)\n",
        "pgd = np.load(path_pgd)\n",
        "from numpy import load\n",
        "PGD = '/content/drive/My Drive/****/Adversarial samples/test_pgd.npy'\n",
        "FGSM = '/content/drive/My Drive/****/Adversarial samples/FGSM0.0001.npy'\n",
        "JSMA = '/content/drive/My Drive/****/Adversarial samples/JSMA.npy'\n",
        "DeepFool = '/content/drive/My Drive/****/Adversarial samples/DeepFool.npy'\n",
        "CWL2 = '/content/drive/My Drive/****/Adversarial samples/carliniL2.npy'\n",
        "BIM = '/content/drive/My Drive/****//Adversarial samples/BIM.npy'\n",
        "FGSM = load(FGSM)\n",
        "fgsm_test = np.reshape(FGSM, (FGSM.shape[0], 113))\n",
        "JSMA = load(JSMA)\n",
        "jsma_test = np.reshape(JSMA, (JSMA.shape[0], 113))\n",
        "BIM = load(BIM)\n",
        "bim_test = np.reshape(BIM, (BIM.shape[0], 113))\n",
        "CWL2 = load(CWL2)\n",
        "cw_test = np.reshape(CWL2, (CWL2.shape[0], 113))\n",
        "DeepFool = load(DeepFool)\n",
        "df_test = np.reshape(DeepFool, (DeepFool.shape[0], 113))\n",
        "PGD = load(PGD)\n",
        "pgd_test = np.reshape(PGD, (PGD.shape[0], 113))\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "import copy\n",
        "import time as time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "pd.options.display.max_columns = 200\n",
        "pd.options.display.max_rows = 200\n",
        "#Downloading and extracting the dataset if it doesn't exist\n",
        "!if [ ! -d \"./NSL-KDD\" ]; then wget http://205.174.165.80/CICDataset/NSL-KDD/Dataset/NSL-KDD.zip; mkdir NSL-KDD; unzip NSL-KDD.zip -d NSL-KDD; fi\n",
        "    \n",
        "#Importing the training and testing datasets from .CSV to Pandas DataFrames\n",
        "features = ['1 Duration', '2 Protocol-type : ', '3 Service : ', '4 Flag : ', '5 Src-bytes', '6 Dst-bytes', '7 Land', '8 Wrong-fragment', '9 Urgent', '10 Hot', '11 Num-failed-logins', '12 Logged-in', '13 Num-compromised', '14 Root-shell', '15 Su-attempted', '16 Num-root', '17 Num-file-creations', '18 Num-shells', '19 Num-access-files', '20 Num-outbound-cmds', '21 Is-host-login', '22 Is-guest-login', '23 Count', '24 Srv-count', '25 Serror-rate', '26 Srv-serror-rate', '27 Rerror-rate', '28 Srv-rerror-rate', '29 Same-srv-rate', '30 Diff-srv-rate', '31 Srv-diff-host-rate', '32 Dst-host-count', '33 Dst-host-srv-count', '34 Dst-host-same-srv-rate', '35 Dst-host-diff-srv-rate', '36 Dst-host-same-src-port-rate', '37 Dst-host-srv-diff-host-rate', '38 Dst-host-serror-rate', '39 Dst-host-srv-serror-rate', '40 Dst-host-rerror-rate', '41 Dst-host-srv-rerror-rate', '42 Attack_type', '43 Difficulty']\n",
        "df_training = pd.read_csv('./NSL-KDD/KDDTrain+.txt', names=features)\n",
        "df_testing = pd.read_csv('./NSL-KDD/KDDTest+.txt', names=features)\n",
        "# Stack the training and test sets\n",
        "data = pd.concat([df_training, df_testing], axis=0)\n",
        "# Dropping features\n",
        "colToDrop = ['43 Difficulty', '20 Num-outbound-cmds','1 Duration', '5 Src-bytes', '6 Dst-bytes','15 Su-attempted','16 Num-root', '17 Num-file-creations', '18 Num-shells', '19 Num-access-files']\n",
        "for col in colToDrop:\n",
        "  data.drop(col, inplace=True, axis=1)\n",
        "  # Transform the nominal attribute \"Attack type\" into binary (0 : normal / 1 : attack)\n",
        "labels = (data['42 Attack_type'] != 'normal').astype('int64')\n",
        "data['42 Labels'] = labels\n",
        "data.drop('42 Attack_type', inplace=True, axis=1)\n",
        "# One Hot Encode the 3 first nominal attributes and drop them\n",
        "for i in ['4 Flag : ', '3 Service : ', '2 Protocol-type : ']:\n",
        "    # Create the One Hot Encode DataFrame\n",
        "    dum = pd.get_dummies(data[i])\n",
        "    # Insert into the dataset DataFrame by Series\n",
        "    for column_name in list(dum.columns):\n",
        "        data.insert(1, str(i)+column_name, dum[column_name])\n",
        "        data[str(i)+column_name] = data[str(i)+column_name].astype('int64')\n",
        "    # Drop the old attribute's column\n",
        "    data.drop(i, inplace=True, axis=1)\n",
        "# Split training and test sets\n",
        "df_training = data[:df_training.shape[0]]    \n",
        "df_testing = data[df_training.shape[0]:]\n",
        "attributes =  ['1 Duration', '5 Src-bytes', '6 Dst-bytes', '8 Wrong-fragment', '9 Urgent', '10 Hot', '11 Num-failed-logins', '13 Num-compromised', '15 Su-attempted', '16 Num-root', '17 Num-file-creations', '18 Num-shells', '19 Num-access-files', '23 Count', '24 Srv-count', '25 Serror-rate', '26 Srv-serror-rate', '27 Rerror-rate', '28 Srv-rerror-rate', '29 Same-srv-rate', '30 Diff-srv-rate', '31 Srv-diff-host-rate', '32 Dst-host-count', '33 Dst-host-srv-count', '34 Dst-host-same-srv-rate', '35 Dst-host-diff-srv-rate', '36 Dst-host-same-src-port-rate', '37 Dst-host-srv-diff-host-rate', '38 Dst-host-serror-rate', '39 Dst-host-srv-serror-rate', '40 Dst-host-rerror-rate', '41 Dst-host-srv-rerror-rate']\n",
        "for col in colToDrop:\n",
        "  if col in attributes:\n",
        "    attributes.remove(col)\n",
        "# Min-Max normalization on the non binary features\n",
        "attributes = ['8 Wrong-fragment','9 Urgent','10 Hot','11 Num-failed-logins','13 Num-compromised','23 Count','24 Srv-count','25 Serror-rate','26 Srv-serror-rate',\n",
        "'27 Rerror-rate','28 Srv-rerror-rate','29 Same-srv-rate','30 Diff-srv-rate','31 Srv-diff-host-rate', '32 Dst-host-count','33 Dst-host-srv-count',\n",
        " '34 Dst-host-same-srv-rate','35 Dst-host-diff-srv-rate','36 Dst-host-same-src-port-rate','37 Dst-host-srv-diff-host-rate','38 Dst-host-serror-rate','39 Dst-host-srv-serror-rate','40 Dst-host-rerror-rate',\n",
        " '41 Dst-host-srv-rerror-rate']\n",
        "for i in attributes:\n",
        "    # The min and max are only computed from the training set\n",
        "    minval = df_training[i].min()\n",
        "    maxval = df_training[i].max()\n",
        "    df_training[i] = ((df_training[i] - minval) / (maxval - minval)) \n",
        "    df_testing[i] = ((df_testing[i] - minval) / (maxval - minval))\n",
        "# Get NumPy arrays from DataFrames\n",
        "nd_training = df_training.values\n",
        "nd_testing = df_testing.values\n",
        "# Separating arguments (x) from lables (y)\n",
        "x_train = nd_training[:, :-1]\n",
        "y_train = nd_training[:, -1]\n",
        "x_test = nd_testing[:, :-1]\n",
        "y_test = nd_testing[:, -1]\n",
        "# Make a copy of the data set as NumPy arrays\n",
        "x_train_np = x_train.copy()\n",
        "y_train_np = y_train.copy()\n",
        "x_test_np = x_test.copy()\n",
        "y_test_np = y_test.copy()\n",
        "# Convert from numpy array to torch tensors\n",
        "x_train = torch.from_numpy(x_train).float()\n",
        "y_train = torch.from_numpy(y_train).long()\n",
        "x_test = torch.from_numpy(x_test).float()\n",
        "y_test = torch.from_numpy(y_test).long()\n",
        "\n",
        "attack_label = (df_training[df_training['42 Labels'] == 1].values)[:, :-1]\n",
        "normal_label = (df_training[df_training['42 Labels'] == 0].values)[:, :-1]\n",
        "\n",
        "attack_label_test = (df_testing[df_testing['42 Labels'] == 1].values)[:, :-1]\n",
        "normal_label_test = (df_testing[df_testing['42 Labels'] == 0].values)[:, :-1]\n",
        "\n",
        "adv_label = np.zeros((50000, 113), dtype=float)\n",
        "adv_label[: 10000] = df[: 10000]\n",
        "adv_label[10000: 20000] = jsma[10000: 20000]\n",
        "adv_label[20000: 30000] = pgd[20000: 30000]\n",
        "adv_label[30000: 40000] = cw[30000: 40000]\n",
        "adv_label[40000: 50000] = fgsm[40000: 50000]\n",
        "\n",
        "adv_label_test = np.zeros((5000, 113), dtype=float)\n",
        "adv_label_test[: 1000] = df_test[: 1000]\n",
        "adv_label_test[1000: 2000] = jsma_test[1000: 2000]\n",
        "adv_label_test[2000: 3000] = pgd_test[2000: 3000]\n",
        "adv_label_test[3000: 4000] = cw_test[3000: 4000]\n",
        "adv_label_test[4000: 5000] = fgsm_test[4000: 5000]\n",
        "\n",
        "normal_label = normal_label[: 50000]\n",
        "attack_label = attack_label[: 50000]\n",
        "x_train_ds = np.zeros((150000, 113), dtype=float)\n",
        "y_train_ds = np.zeros((150000, 1), dtype=float)\n",
        "indices = np.arange(150000)\n",
        "np.random.shuffle(indices)\n",
        "for i in range(150000):\n",
        "  if i < 50000:\n",
        "    x_train_ds[indices[i]] = normal_label[i%50000]\n",
        "    y_train_ds[indices[i]] = 0\n",
        "  elif i>50000 and i< 100000:\n",
        "    x_train_ds[indices[i]] = attack_label[i%50000]\n",
        "    y_train_ds[indices[i]] = 1\n",
        "  else:\n",
        "    x_train_ds[indices[i]] = adv_label[i%50000]\n",
        "    y_train_ds[indices[i]] = 2\n",
        "\n",
        "normal_label_test = normal_label_test[: 9000]\n",
        "attack_label_test = attack_label_test[: 12000]\n",
        "x_test_ds = np.zeros((26000, 113), dtype=float)\n",
        "y_test_ds = np.zeros((26000, 1), dtype=float)\n",
        "indices = np.arange(26000)\n",
        "np.random.shuffle(indices)\n",
        "for i in range(26000):\n",
        "  if i < 12000:\n",
        "    x_test_ds[indices[i]] = attack_label_test[i%12000]\n",
        "    y_test_ds[indices[i]] = 1\n",
        "  elif i>12000 and i< 21000:\n",
        "    x_test_ds[indices[i]] = normal_label_test[i%9000]\n",
        "    y_test_ds[indices[i]] = 0\n",
        "  else:\n",
        "    x_test_ds[indices[i]] = adv_label_test[i%5000]\n",
        "    y_test_ds[indices[i]] = 2\n",
        "\n",
        "!pip install pytorch-tabnet\n",
        "import torch\n",
        "from torchvision.models.resnet import resnet50\n",
        "from torchsummary import summary\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f'Using {device} for inference')\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "clf = TabNetClassifier()  \n",
        "\n",
        "\n",
        "y_train_ds = np.reshape(y_train_ds, (150000))\n",
        "y_test_ds = np.reshape(y_test_ds, (26000))\n",
        "\n",
        "\n",
        "clf.fit(\n",
        "  x_train_ds, y_train_ds,\n",
        "  eval_set=[(x_test_ds, y_test_ds)]\n",
        ")\n",
        "\n",
        "def prediction(model, data):\n",
        "  preds = model.predict(data)\n",
        "  preds2 = model.predict(data)\n",
        "  pred = preds2[0].astype(int)\n",
        "  return np.argmax(model.predict_proba(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NIFVUMt1JQ8",
        "outputId": "5cf4aa77-5fee-4117-b5bf-fdb7b883359a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "--2022-06-30 10:43:17--  http://205.174.165.80/CICDataset/NSL-KDD/Dataset/NSL-KDD.zip\n",
            "Connecting to 205.174.165.80:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6598776 (6.3M) [application/zip]\n",
            "Saving to: ‘NSL-KDD.zip’\n",
            "\n",
            "NSL-KDD.zip         100%[===================>]   6.29M  1.74MB/s    in 3.9s    \n",
            "\n",
            "2022-06-30 10:43:23 (1.61 MB/s) - ‘NSL-KDD.zip’ saved [6598776/6598776]\n",
            "\n",
            "Archive:  NSL-KDD.zip\n",
            "  inflating: NSL-KDD/index.html      \n",
            "  inflating: NSL-KDD/KDDTest1.jpg    \n",
            "  inflating: NSL-KDD/KDDTest-21.arff  \n",
            "  inflating: NSL-KDD/KDDTest-21.txt  \n",
            "  inflating: NSL-KDD/KDDTest+.arff   \n",
            "  inflating: NSL-KDD/KDDTest+.txt    \n",
            "  inflating: NSL-KDD/KDDTrain1.jpg   \n",
            "  inflating: NSL-KDD/KDDTrain+.arff  \n",
            "  inflating: NSL-KDD/KDDTrain+.txt   \n",
            "  inflating: NSL-KDD/KDDTrain+_20Percent.arff  \n",
            "  inflating: NSL-KDD/KDDTrain+_20Percent.txt  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:97: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:98: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-tabnet\n",
            "  Downloading pytorch_tabnet-3.1.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.4.1)\n",
            "Requirement already satisfied: torch<2.0,>=1.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.21.6)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.36 in /usr/local/lib/python3.7/dist-packages (from pytorch-tabnet) (4.64.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2.0,>=1.2->pytorch-tabnet) (4.1.1)\n",
            "Installing collected packages: pytorch-tabnet\n",
            "Successfully installed pytorch-tabnet-3.1.1\n",
            "Using cpu for inference\n",
            "Device used : cpu\n",
            "epoch 0  | loss: 0.48782 | val_0_accuracy: 0.44562 |  0:00:10s\n",
            "epoch 1  | loss: 0.31465 | val_0_accuracy: 0.47142 |  0:00:21s\n",
            "epoch 2  | loss: 0.30327 | val_0_accuracy: 0.50219 |  0:00:32s\n",
            "epoch 3  | loss: 0.2815  | val_0_accuracy: 0.726   |  0:00:43s\n",
            "epoch 4  | loss: 0.26539 | val_0_accuracy: 0.73535 |  0:00:53s\n",
            "epoch 5  | loss: 0.25717 | val_0_accuracy: 0.73115 |  0:01:05s\n",
            "epoch 6  | loss: 0.25352 | val_0_accuracy: 0.73277 |  0:01:15s\n",
            "epoch 7  | loss: 0.25111 | val_0_accuracy: 0.74677 |  0:01:26s\n",
            "epoch 8  | loss: 0.25517 | val_0_accuracy: 0.72538 |  0:01:36s\n",
            "epoch 9  | loss: 0.2449  | val_0_accuracy: 0.72946 |  0:01:46s\n",
            "epoch 10 | loss: 0.2421  | val_0_accuracy: 0.71381 |  0:01:57s\n",
            "epoch 11 | loss: 0.24442 | val_0_accuracy: 0.42077 |  0:02:07s\n",
            "epoch 12 | loss: 0.28418 | val_0_accuracy: 0.70885 |  0:02:17s\n",
            "epoch 13 | loss: 0.29565 | val_0_accuracy: 0.505   |  0:02:27s\n",
            "epoch 14 | loss: 0.28298 | val_0_accuracy: 0.22704 |  0:02:38s\n",
            "epoch 15 | loss: 0.45179 | val_0_accuracy: 0.43612 |  0:02:48s\n",
            "epoch 16 | loss: 0.38594 | val_0_accuracy: 0.66635 |  0:02:59s\n",
            "epoch 17 | loss: 0.39184 | val_0_accuracy: 0.50558 |  0:03:09s\n",
            "\n",
            "Early stopping occurred at epoch 17 with best_epoch = 7 and best_val_0_accuracy = 0.74677\n",
            "Best weights from best epoch are automatically used!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading The Data"
      ],
      "metadata": {
        "id": "tnpTUDl62Pj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hsja = \"/content/drive/My Drive/****/Adversarial samples/hsjaL2Attack.npy\"\n",
        "hsja = load(hsja)\n",
        "hsja = np.reshape(hsja, (hsja.shape[0], 113))\n",
        "\n",
        "Attacks = '/content/drive/My Drive/****/Adversarial samples/Attack_Class.npy'\n",
        "Attacks = np.load(Attacks)\n",
        "Attacks = np.reshape(Attacks, (Attacks.shape[0], 113))\n",
        "\n",
        "\n",
        "path = '/content/drive/My Drive/****/Adversarial samples/Boundary/boundary_full.npy'\n",
        "boundary = np.loadtxt(path)\n",
        "\n",
        "Normal = '/content/drive/My Drive/****/Adversarial samples/Benign_Class.npy'\n",
        "Normal = np.loadtxt(Normal)"
      ],
      "metadata": {
        "id": "0uuoqBjW2Rhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading The NIDS"
      ],
      "metadata": {
        "id": "X26HCHZ-QJOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model500Path = \"/content/drive/My Drive/****/content/LSTMNIDS500/content/LSTM-NIDS-500\"\n",
        "model = tf.saved_model.load(model500Path)\n",
        "'''\n",
        "Makes prediction on x \n",
        "return True if x is of label y\n",
        "'''\n",
        "def predict(x):\n",
        "  inp = tf.convert_to_tensor(x)\n",
        "  inp = tf.cast(inp, tf.float32)\n",
        "  inp = tf.reshape(inp, ( 1, 1, 113))\n",
        "  labeling = model(inp)\n",
        "  return np.argmax(labeling)"
      ],
      "metadata": {
        "id": "FKyfVmUOQMAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating Aversarial Mix Of Data\n",
        "<img src = \"https://drive.google.com/uc?id=1DbbnVcYLC2NCyMjmeaSXV96UV7zidg4n\" height = \"400\">"
      ],
      "metadata": {
        "id": "OvOMWGif5fv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "indices = random.sample(range(48093), 48093)\n",
        "# Labels : 0 Normal, 1 attack, 2 boundary, 3 hsja\n",
        "Labels = np.zeros((48093, 1), int)\n",
        "Mix = np.zeros((48093, 113), float)\n",
        "indices1 = indices[0: 9594]\n",
        "indices2 = indices[9594 : 22427]\n",
        "indices3 = indices[22427 : 35260]\n",
        "indices4 = indices[35260 : 48093]\n",
        "for i in range(Normal.shape[0]):\n",
        "  Mix[indices1[i]] = Normal[i]\n",
        "  Labels[indices1[i]] = 0\n",
        "for i in range(Attacks.shape[0]):\n",
        "  Mix[indices2[i]] = Attacks[i]\n",
        "  Labels[indices2[i]] = 1\n",
        "for i in range(boundary.shape[0]):\n",
        "  Mix[indices3[i]] = boundary[i]\n",
        "  Labels[indices3[i]] = 2\n",
        "for i in range(hsja.shape[0]):\n",
        "  Mix[indices4[i]] = hsja[i]\n",
        "  Labels[indices4[i]] = 3"
      ],
      "metadata": {
        "id": "a9pWRi4E9wlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Mix Of Bening Data\n",
        "<img src = \"https://drive.google.com/uc?id=109u40oGbUw43kJF7dLjOK_ItZTIADDgQ\" height = \"400\">\n"
      ],
      "metadata": {
        "id": "jOq3WhgukFMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indices = random.sample(range(22427),22427)\n",
        "from_dataset_data = np.zeros((22427, 113), float)\n",
        "from_dataset_labels = np.zeros((22427, 1), int)\n",
        "indices1 = indices[0: 9594]\n",
        "indices2 = indices[9594 : 22427]\n",
        "for i in range(Normal.shape[0]):\n",
        "  from_dataset_data[indices1[i]] = Normal[i]\n",
        "  from_dataset_labels[indices1[i]] = 0\n",
        "for i in range(Attacks.shape[0]):\n",
        "  from_dataset_data[indices2[i]] = Attacks[i]\n",
        "  from_dataset_labels[i] = 1"
      ],
      "metadata": {
        "id": "zZrZGTSOk6ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating The Test"
      ],
      "metadata": {
        "id": "fx7N5IIz_vXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "def test_stat(sample):\n",
        "  # Generate Random Normal sample\n",
        "  ben_sample = samplestats(Normal,Normal,Normal.shape[0], Normal.shape[0],1.0)\n",
        "  ben_sample = ben_sample[: 10]\n",
        "  ret = compute_stats(ben_sample,sample,10, 10,ratio=1.0,runs=10)\n",
        "  if mean(ret) < 0.05:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "metadata": {
        "id": "qZn7V6oW_pRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 1\n",
        "<img src = \"https://drive.google.com/uc?id=1rhqP3_jpFfvfPTotOqyqXPl8XJ5q57tT\" height = \"400\">\n"
      ],
      "metadata": {
        "id": "6n94WpJWMDxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 1: Boundary\n",
        "import time\n",
        "start_time = time.time()\n",
        "preds = []\n",
        "detected_sam = 0\n",
        "for i in range(0, 100, 10):\n",
        "  sam = boundary[i:i+10]\n",
        "  ret = test_stat(sam)\n",
        "  if ret:\n",
        "    detected_sam += 1\n",
        "  else:\n",
        "    for j in range(10):\n",
        "      pred = predict(sam[j])\n",
        "      preds.append(pred)\n",
        "\n",
        "print('Detection rate of Boudnary sample is ', detected_sam*10, \"%\")\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6p9C3OmIlWk",
        "outputId": "d207bcbf-6ca3-4961-9804-8ebb7eaaf5fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detection rate of Boudnary sample is  100 %\n",
            "--- 21.20804452896118 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 1: HSJA\n",
        "import time\n",
        "start_time = time.time()\n",
        "preds = []\n",
        "detected_sam = 0\n",
        "for i in range(0, 100, 10):\n",
        "  sam = hsja[i:i+10]\n",
        "  ret = test_stat(sam)\n",
        "  if ret:\n",
        "    detected_sam += 1\n",
        "  else:\n",
        "    for j in range(10):\n",
        "      pred = predict(sam[j])\n",
        "      preds.append(pred)\n",
        "\n",
        "print('Detection rate of HopSkipJump sample is ', detected_sam*10, \"%\")\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzEFaqlwJtU3",
        "outputId": "33c887ce-6f4c-46fe-99a9-a928181713ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detection rate of HopSkipJump sample is  100 %\n",
            "--- 21.777459859848022 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 2\n",
        "<img src = \"https://drive.google.com/uc?id=1TqQntxEdlb-2JHz9ceoE9nBWajkSHr6R\" height = \"400\">\n"
      ],
      "metadata": {
        "id": "cogt650XMm1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 2: Boundary\n",
        "import time\n",
        "start_time = time.time()\n",
        "preds = []\n",
        "detected_sam = 0\n",
        "recovered_sam = 0\n",
        "missed_sam = 0\n",
        "NIDS_det = 0\n",
        "NIDS_mis = 0\n",
        "for i in range(0, 100, 1):\n",
        "  sam = boundary[i]\n",
        "  sam = np.reshape(sam, (1, 113))\n",
        "  pred = prediction(clf, sam)\n",
        "  if pred == 0:\n",
        "    missed_sam += 1\n",
        "    predn = predict(sam)\n",
        "    preds.append(predn)\n",
        "    if predn == 1:\n",
        "      NIDS_det += 1\n",
        "    else:\n",
        "      NIDS_mis += 1\n",
        "  elif pred == 1:\n",
        "    recovered_sam += 1\n",
        "  else:\n",
        "    detected_sam += 1\n",
        "\n",
        "print(\"Attack Boundary\")\n",
        "print(\"detectection rate by outlier \", detected_sam, \"%\")\n",
        "print(\"recovery rate by outlier \", recovered_sam, \"%\")\n",
        "print(\"error rate by outlier \", missed_sam, \"%\")\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))  \n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djGSgnZjtDhx",
        "outputId": "6c4563ac-1038-4ac3-999e-3b614854f4b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attack Boundary\n",
            "detectection rate by outlier  32 %\n",
            "recovery rate by outlier  55 %\n",
            "error rate by outlier  13 %\n",
            "--- 2.033456563949585 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 2: HSJA\n",
        "import time\n",
        "start_time = time.time()\n",
        "preds = []\n",
        "detected_sam = 0\n",
        "recovered_sam = 0\n",
        "missed_sam = 0\n",
        "NIDS_det = 0\n",
        "NIDS_mis = 0\n",
        "for i in range(0, 100, 1):\n",
        "  sam = hsja[i]\n",
        "  sam = np.reshape(sam, (1, 113))\n",
        "  pred = prediction(clf, sam)\n",
        "  if pred == 0:\n",
        "    missed_sam += 1\n",
        "    predn = predict(sam)\n",
        "    preds.append(predn)\n",
        "    if predn == 1:\n",
        "      NIDS_det += 1\n",
        "    else:\n",
        "      NIDS_mis += 1\n",
        "  elif pred == 1:\n",
        "    recovered_sam += 1\n",
        "  else:\n",
        "    detected_sam += 1\n",
        "\n",
        "print(\"Attack HopSKipJump\")\n",
        "print(\"detectection rate by outlier \", detected_sam, \"%\")\n",
        "print(\"recovery rate by outlier \", recovered_sam, \"%\")\n",
        "print(\"error rate by outlier \", missed_sam, \"%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5Xu2y3p4k8c",
        "outputId": "1f623de5-3197-47f4-d0d6-cdb3304f52ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attack HopSKipJump\n",
            "detectection rate by outlier  82 %\n",
            "recovery rate by outlier  12 %\n",
            "error rate by outlier  6 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Sample():          \n",
        "    def __init__(self, sample, isAdv, labels):   \n",
        "        self.sample = sample\n",
        "        self.isAdv = isAdv\n",
        "        self.labels = labels"
      ],
      "metadata": {
        "id": "HC9SYrbBEPWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting real predictions of samples\n",
        "start = 0\n",
        "areAdv = [False] * 4809\n",
        "indice = 0\n",
        "samples = []\n",
        "for i in range(10, 48090, 10):\n",
        "  sample = Mix[start: start + 10]\n",
        "  sample_lab = Labels[start: start + 10]\n",
        "  start = i\n",
        "  isAdv = False\n",
        "  for j in range(10):\n",
        "    if sample_lab[j] == 2 or sample_lab[j] == 3:\n",
        "      isAdv = True\n",
        "  if isAdv:\n",
        "    areAdv[indice] = True\n",
        "  else:\n",
        "    areAdv[indice] = False\n",
        "  obj = Sample(sample, areAdv[indice] , sample_lab)\n",
        "  samples.append(obj)\n",
        "  indice += 1"
      ],
      "metadata": {
        "id": "qlc-iqXWAk6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating benign samples\n",
        "start = 0\n",
        "benn_samples = [] \n",
        "for i in range(0, 22420, 10):\n",
        "  portion = from_dataset_data[start: start + 10]\n",
        "  lab = from_dataset_labels[start: start + 10]\n",
        "  #print(i,portion.shape, lab.shape )\n",
        "  start += 10\n",
        "  s = Sample(portion, False, lab )\n",
        "  #print(s)\n",
        "  benn_samples.append(s)"
      ],
      "metadata": {
        "id": "fm0KbYDhJ2Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the two samples\n",
        "merged_samples = [None] * 7050\n",
        "indices = random.sample(range(7050), 7050)\n",
        "indicesben = indices[0:2242 ]\n",
        "indicesadv = indices[2242: 7050]\n",
        "for i in range(len(indicesben)):\n",
        "  #print(i, indicesben[i])\n",
        "  merged_samples[indicesben[i]] = benn_samples[i]\n",
        "for i in range(len(indicesadv)):\n",
        "  merged_samples[indicesadv[i]] = samples[i]\n"
      ],
      "metadata": {
        "id": "vDnvLURgKftq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 3\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1u5L5sEio6NvDiqfneW9anHMGSoxvWEDa\" height = \"400\">\n"
      ],
      "metadata": {
        "id": "9xx_9D_QQexI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "vp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "vn = 0\n",
        "detect_rate = 0\n",
        "rec_rate = 0\n",
        "error_rate = 0\n",
        "detection_nids = 0\n",
        "error_nids = 0\n",
        "number =  100 #len(merged_samples)\n",
        "adv_num = 0\n",
        "adv_num_nids = 0\n",
        "activ_test_stat = True\n",
        "activ_outlier = True\n",
        "for i in range(number):\n",
        "  passs = False\n",
        "  data = merged_samples[i]\n",
        "  sam = data.sample\n",
        "  if activ_test_stat:\n",
        "    ret = test_stat(sam)\n",
        "    if ret and data.isAdv :\n",
        "      vp += 1\n",
        "    elif ret and not data.isAdv:\n",
        "      fp += 1\n",
        "    elif not ret and data.isAdv :\n",
        "      fn += 1\n",
        "    elif not ret and not data.isAdv:\n",
        "      vn += 1\n",
        "\n",
        "  for j in range(10):\n",
        "    if data.labels[j] == 2 or data.labels[j] == 3:\n",
        "      adv_num += 1\n",
        "  if (ret and activ_outlier) or (not activ_test_stat and activ_outlier):\n",
        "    passs = True\n",
        "    for i in range(10):\n",
        "      reshaped_data = np.reshape(sam[i], (1, 113))\n",
        "      pred = prediction(clf, reshaped_data)\n",
        "      if pred == 2 and (data.labels[i] == 2  or data.labels[i] == 3 ):\n",
        "        detect_rate += 1\n",
        "      elif pred == 1 and (data.labels[i] == 2  or data.labels[i] == 3 ):\n",
        "        rec_rate += 1\n",
        "      elif pred == 0 and (data.labels[i] == 2  or data.labels[i] == 3 ):\n",
        "        error_rate += 1\n",
        "\n",
        "      if pred == 0: \n",
        "        pred_nids = predict( sam[i])\n",
        "        if data.labels[i] != 0:\n",
        "          adv_num_nids += 1\n",
        "        if pred_nids == 1 and data.labels[i] != 0:\n",
        "          detection_nids += 1\n",
        "        elif pred_nids == 0 and data.labels[i] != 0:\n",
        "          error_nids += 1\n",
        "  if (not ret and  activ_test_stat) or (not activ_test_stat and not activ_outlier) or not passs :\n",
        "    for i in range(10):\n",
        "      pred_nids = predict(sam[i])\n",
        "      if data.labels[i] != 0:\n",
        "          adv_num_nids += 1\n",
        "      if pred_nids == 1 and data.labels[i] != 0:\n",
        "        detection_nids += 1\n",
        "      elif pred_nids == 0 and data.labels[i] != 0:\n",
        "        error_nids += 1\n",
        "         \n",
        "print(\"True positives  Stat Test \",(vp/number) * 100)\n",
        "print(\"False positives Stat Test \", (fp/number) * 100)\n",
        "print(\"True negatives Stat Test\", (vn/number) * 100)\n",
        "print(\"False negatives Stat Test \", (fn/number) * 100)\n",
        "print(\"outlier detection rate \", (detect_rate/adv_num) * 100 )\n",
        "print(\"outlier recovery rate \", (rec_rate/adv_num) * 100 )\n",
        "print(\"outlier error rate \", (error_rate/adv_num) * 100 )\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RisxjQTtQgYy",
        "outputId": "4f947a51-5b6d-46f7-e1ab-fa3ad040b33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True positives  Stat Test  48.0\n",
            "False positives Stat Test  25.0\n",
            "True negatives Stat Test 13.0\n",
            "False negatives Stat Test  14.000000000000002\n",
            "outlier detection rate  43.93939393939394\n",
            "outlier recovery rate  26.36363636363636\n",
            "outlier error rate  8.787878787878787\n",
            "--- 225.1943597793579 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END"
      ],
      "metadata": {
        "id": "caLPtkxaMLLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Results for 10 samples\n",
        "test_stat = True, outlier_class = True\n",
        "\n",
        "True positives  Stat Test  0.7\n",
        "False positives Stat Test  0.1\n",
        "True negatives Stat Test 0.1\n",
        "False negatives Stat Test  0.1\n",
        "outlier detection rate  0.38461538461538464\n",
        "outlier recovery rate  0.38461538461538464\n",
        "outlier error rate  0.1794871794871795\n",
        "The nids detection  0.4827586206896552\n",
        "The nids error  0.5172413793103449\n",
        "--- 35.840468645095825 seconds ---\n",
        "\n",
        "test_stat = False, outlier_class = True\n",
        "True positives  Stat Test  0.0\n",
        "False positives Stat Test  0.0\n",
        "True negatives Stat Test 0.0\n",
        "False negatives Stat Test  0.0\n",
        "outlier detection rate  43.58974358974359\n",
        "outlier recovery rate  38.46153846153847\n",
        "outlier error rate  17.94871794871795\n",
        "The nids detection  57.89473684210527\n",
        "The nids error  42.10526315789473\n",
        "--- 3.3734238147735596 seconds ---\n",
        "\n",
        "test_stat = True, outlier_class = False\n",
        "True positives  Stat Test  60.0\n",
        "False positives Stat Test  10.0\n",
        "True negatives Stat Test 10.0\n",
        "False negatives Stat Test  20.0\n",
        "outlier detection rate  0.0\n",
        "outlier recovery rate  0.0\n",
        "outlier error rate  0.0\n",
        "The nids detection  34.5679012345679\n",
        "The nids error  65.4320987654321\n",
        "--- 28.26953148841858 seconds ---\n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "error_nids = 0\n",
        "number = 1000\n",
        "adv_num = 0\n",
        "adv_num_nids = 0\n",
        "activ_test_stat = True\n",
        "activ_outlier = True\n",
        "for i in range(number):\n",
        "  passs = False\n",
        "  data = merged_samples[i]\n",
        "  sam = data.sample\n",
        "\n",
        "True positives  Stat Test  65.5\n",
        "False positives Stat Test  18.7\n",
        "True negatives Stat Test 11.899999999999999\n",
        "False negatives Stat Test  3.9\n",
        "outlier detection rate  32.11875843454791\n",
        "outlier recovery rate  41.75438596491228\n",
        "outlier error rate  22.53711201079622\n",
        "The nids detection  36.69603524229075\n",
        "The nids error  63.30396475770925\n",
        "--- 2342.1765415668488 seconds ---\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tvyQMECmnhi",
        "outputId": "d694a7f8-fa9a-4589-b7dc-78f650a03d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    }
  ]
}